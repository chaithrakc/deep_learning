{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSZX/azMo25UBXvKQpzO+9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer Network"],"metadata":{"id":"Lqys83YatfzL"}},{"cell_type":"markdown","source":["The research paper \"Attention Is All You Need\" published in 2017 introduced Transformer, which has brought about a revolutionary change in the field of NLP.\n","\n","\n","**Research Paper:** https://arxiv.org/abs/1706.03762\n","\n","Why Trasnformer?\n","\n","Key things that makes transformer better than RNN, LSTM RNN, Attention based LSTM RNN:\n","- Parellel processing of the inputs which can make the training process lot faster\n","- Better contextual representation of the input because of multi-head self attentions\n","\n","When to use Transformer?\n"],"metadata":{"id":"DHVuC6uYs6xt"}},{"cell_type":"markdown","source":["## Table of Contents\n","\n","- [Python Libraries](#0)\n","- [1 - Positional Encoding](#1)\n","- [2 - Masking](#2)\n","    - [2.1 - Padding Mask](#2-1)\n","    - [2.2 - Look-ahead Mask](#2-2)\n","- [3 - Encoder](#4)\n","    - [4.1 Encoder Layer](#4-1)\n","    - [4.2 - Full Encoder](#4-2)\n","- [4 - Decoder](#5)\n","    - [5.1 - Decoder Layer](#5-1)\n","    - [5.2 - Full Decoder](#5-2)\n","- [5 - Transformer](#6)\n","- [6 - References](#7)"],"metadata":{"id":"WZzBYuhNuhlh"}},{"cell_type":"markdown","source":["<a name='0'></a>\n","## Python Libraries\n","\n","Loading all the required python packages."],"metadata":{"id":"hSGwUOqSutYo"}},{"cell_type":"code","execution_count":46,"metadata":{"id":"9FjVj0jHs1p-","executionInfo":{"status":"ok","timestamp":1677734392742,"user_tz":300,"elapsed":126,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n","#one hot representation\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Positional Encoding\n","\n","Positional encoding to the input sequence is a critical step in the Transformer architecture, as it enables the model to understand the sequential order of the tokens in the input sequence.\n","\n","The formula for calculating the positional encoding is as follows:\n","\n","$${PE}{(pos,k)} = sin(pos/10000^{2i/dmodel})$$\n","\n","$${PE}{(pos,k+1)} = cos(pos/10000^{2i/dmodel})$$\n","\n","- $pos$ is the position of the token in the sequence\n","- $k = 2i$ is the index of the the each dimension in positional encoding. So, $i= k//2$\n","- $dmodel$ is the dimension of the embedding vector.\n","\n","if $k=[0,1,2,3,4,5]$ indices of postional embedding, then $i=[0,0,1,1,2,2]$"],"metadata":{"id":"PnoR1txKwiCs"}},{"cell_type":"code","source":["def get_angles(pos, k, d_model:int):\n","  \"\"\"\n","  Arguments:\n","  pos -- (an array of shape (position, 1) representing the positions in the sequence\n","  k -- k (an array of shape (1, d_model) representing the indices of the embedding dimensions\n","  d_model -- integer representing the dimensionality of the model\n","  \n","  Returns:\n","  angles -- an array of shape (position, d_model) representing the angles for the positional encoding.\n","  \"\"\"\n","  i = k//2\n","  angles = pos/np.power(10_000, 2*i/d_model)\n","  return angles"],"metadata":{"id":"pNUKgkkJyHBU","executionInfo":{"status":"ok","timestamp":1677734393063,"user_tz":300,"elapsed":148,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position:int, d_model:int ):\n","  \"\"\"\n","  Arguments:\n","  position: an integer indicating the maximum sequence length\n","  d_model: an integer indicating the dimensionality of the model\n","  \n","  Returns:\n","  encoding -- a 3D tensor of shape (1, position, d_model) representing the positional encoding for a sequence of length position\n","  \"\"\"\n","  angle_radians = get_angles(np.arange(position)[: , np.newaxis],\n","                             np.arange(d_model)[np.newaxis, :],\n","                             d_model)\n","  # apply sin to even indices in the array; 2i\n","  angle_radians[:, 0::2] = np.sin(angle_radians[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_radians[:,1:2] = np.cos(angle_radians[:, 1:2])\n","\n","  pos_encoding = angle_radians[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"lOqaDq8NrRx-","executionInfo":{"status":"ok","timestamp":1677734393063,"user_tz":300,"elapsed":9,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["<a name='2'></a>\n","## 2 - Masking\n","\n","There are two types of masks used when building Transformer network: *padding mask* and *look-ahead mask*. \n","\n","<a name='2-1'></a>\n","### 2.1 - Padding Mask\n","\n","It is important to feed sequences of uniform length to the transformer. We can pad the sequences with zeros, and truncate sequences that exceed maximum length of the model."],"metadata":{"id":"_GWLf9Tw6YyM"}},{"cell_type":"code","source":["def create_padding_mask(seq):\n","  \"\"\"\n","    Creates a mask tensor representing the padding positions in the input sequence.\n","    \n","    Arguments:\n","    seq -- a tensor of shape (batch_size, seq_len)\n","\n","    Returns:\n","    mask -- a tensor of shape (batch_size, 1, seq_len), where each position is 0 if the corresponding position in\n","    the input sequence is a padding position, and 1 otherwise.\n","  \"\"\"\n","  mask = 1 - tf.cast(tf.math.equal(seq, 0),dtype=tf.float32)\n","\n","  # reshaping mask so that it has an additional dimension, \n","  # which will be needed when applying the mask in the self-attention mechanism of the Transformer model. \n","  return mask[:, tf.newaxis, :]\n"],"metadata":{"id":"GvwNpZ8j7Put","executionInfo":{"status":"ok","timestamp":1677734393064,"user_tz":300,"elapsed":9,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["<a name='2-2'></a>\n","### 2.2 - Look-ahead Mask\n"],"metadata":{"id":"YJOvyBOs7NOO"}},{"cell_type":"code","source":["def create_look_ahead_mask(sequence_length):\n","  mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n","  return mask "],"metadata":{"id":"LnT4HTMl7QYk","executionInfo":{"status":"ok","timestamp":1677734393064,"user_tz":300,"elapsed":9,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["<a name='4'></a>\n","## 3 - Encoder\n","\n","Encoder contains - multi-head self attention layers and feed forward neural network that is independently applied to every position."],"metadata":{"id":"PO1-1ErWOC0k"}},{"cell_type":"code","source":["def FeedForward(embedding_dim, full_connected_dim):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(full_connected_dim, activation='relu'),\n","      tf.keras.layers.Dense(embedding_dim)\n","  ])"],"metadata":{"id":"Uvx9UkMAOIYj","executionInfo":{"status":"ok","timestamp":1677734393065,"user_tz":300,"elapsed":10,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, full_connected_dim,dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, full_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon = layernorm_eps )\n","    self.layer_norm2 = LayerNormalization(epsilon = layernorm_eps)\n","    self.drop_out = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    self_mha_output = self.mha(x,x,x,mask)\n","    out1 = self.layer_norm1(x + self_mha_output)\n","    ffn_output = self.ffnn(out1)\n","    ffn_output = self.drop_out(ffn_output, training=training)\n","    encoder_layer_out = self.layer_norm2(out1 + ffn_output)\n","    return encoder_layer_out\n"],"metadata":{"id":"MxBFDD4pQ89U","executionInfo":{"status":"ok","timestamp":1677734393065,"user_tz":300,"elapsed":9,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_encoders, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","               max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_encoders\n","    self.embedding = Embedding(input_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.enc_layers = [EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate, layernorm_eps) for _ in range(num_encoders)]\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    seq_len = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","    return x"],"metadata":{"id":"k_6gxc_2pF7g","executionInfo":{"status":"ok","timestamp":1677734393066,"user_tz":300,"elapsed":10,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["<a name='5'></a>\n","## 4 - Decoder\n"],"metadata":{"id":"myl_skWyOIqn"}},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.masked_mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, fully_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm2 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm3 = LayerNormalization(epsilon=layernorm_eps)\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    mult_attn_out1, attn_weights_block1 = self.masked_mha(x, x, x, look_ahead_mask, return_attention_scores=True)\n","    Q1 = self.layer_norm1(mult_attn_out1 + x)\n","    mult_attn_out2, attn_weights_block2 = self.mha(Q1, enc_output, enc_output, padding_mask, return_attention_scores=True) \n","    mult_attn_out2 = self.layer_norm2(mult_attn_out2 + Q1)\n","    ffn_output = self.ffnn(mult_attn_out2)\n","    ffn_output = self.dropout(ffn_output, training = training)\n","    out3 = self.layer_norm3(ffn_output + mult_attn_out2)\n","    return out3, attn_weights_block1, attn_weights_block2\n","    "],"metadata":{"id":"nQFSLdQ-OM89","executionInfo":{"status":"ok","timestamp":1677734393067,"user_tz":300,"elapsed":10,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_decoders, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_decoders\n","    self.embedding = Embedding(target_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.dec_layers = [DecoderLayer(embedding_dim, num_heads, fully_connected_dim) for _ in range(num_decoders)]\n","    self.dropout = Dropout(dropout_rate)\n","\n","  def __call__(self,x, enc_output, training, look_ahead_mask, padding_mask):\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    x = self.embedding(x)\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","      attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n","    return x, attention_weights"],"metadata":{"id":"bX0w0MixtG7e","executionInfo":{"status":"ok","timestamp":1677734393188,"user_tz":300,"elapsed":131,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["<a name='6'></a> \n","## 5 - Transformer\n"],"metadata":{"id":"ISOTSziqONhq"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, \n","               max_pos_encoding_input, max_pos_encoding_target):\n","    super().__init__()\n","    self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_pos_encoding_input)\n","    self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding_target)\n","    self.final_layer = Dense(target_vocab_size, activation='softmax')\n","  \n","  def __call__(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","    enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n","    dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask )\n","    final_output = self.final_layer(dec_output)\n","    return final_output, attention_weights"],"metadata":{"id":"Cy6wxAFqOOUn","executionInfo":{"status":"ok","timestamp":1677734393189,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["Machine Translation (English to French) using the above transformer"],"metadata":{"id":"OqexyS2prDmV"}},{"cell_type":"code","source":["!!curl -O http://www.manythings.org/anki/fra-eng.zip\n","!!unzip fra-eng.zip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQITwmyJOkUr","executionInfo":{"status":"ok","timestamp":1677734439041,"user_tz":300,"elapsed":45856,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"89ce855a-b468-4e15-ea9b-c8d5e54cb45b"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Archive:  fra-eng.zip',\n"," 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n',\n"," 'replace fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n']"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["data_path = \"fra.txt\"\n","batch_size= 10_000\n","input_texts = []\n","target_texts = []\n","input_vocab = set()\n","target_vocab = set()\n","\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","    \n","for line in lines[: min(batch_size, len(lines) - 1)]:\n","  line = line.lower()\n","  input_text, target_text, _ = line.split(\"\\t\")\n","  input_texts.append(input_text)\n","  target_texts.append(target_text)\n","\n","  for word in input_text.lower().split():\n","    if word not in input_vocab:\n","      input_vocab.add(word)\n","\n","  for word in target_text.split():\n","    if word not in target_vocab:\n","      target_vocab.add(word)\n"],"metadata":{"id":"GpnNbdqlOqbe","executionInfo":{"status":"ok","timestamp":1677734508891,"user_tz":300,"elapsed":551,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["input_vocab_sroted = sorted(list(input_vocab))\n","target_vocab_sroted = sorted(list(target_vocab))\n","\n","input_vocab_sroted.append('pad')\n","target_vocab_sroted.append('pad')\n","\n","ENCODER_VOCAB_SIZE = len(input_vocab_sroted)\n","TARGET_VOCAB_SIZE = len(target_vocab_sroted)\n","\n","# needed for padding\n","max_encoder_seq_length = max([len(sentence.split()) for sentence in input_texts])\n","max_decoder_seq_length = max([len(sentence.split()) for sentence in target_texts])\n","\n","print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", ENCODER_VOCAB_SIZE)\n","print(\"Number of unique target tokens:\", TARGET_VOCAB_SIZE)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP4E7qu8PFcA","executionInfo":{"status":"ok","timestamp":1677734594232,"user_tz":300,"elapsed":535,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"4c536565-85f4-4c68-db4a-0bf0ea30c633"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 10000\n","Number of unique input tokens: 2727\n","Number of unique target tokens: 5391\n","Max sequence length for inputs: 4\n","Max sequence length for outputs: 10\n"]}]},{"cell_type":"code","source":["# adding index to each character in the sorted list\n","input_token_index = dict([(word, i) for i, word in enumerate(input_vocab_sroted)])\n","output_token_index = dict([(word, i) for i, word in enumerate(target_vocab_sroted)])"],"metadata":{"id":"PbhivCQZoUeb","executionInfo":{"status":"ok","timestamp":1677734596338,"user_tz":300,"elapsed":2,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["# intializing one hot encode vectors\n","\n","# encoder_input_data is a 3D array of shape (num_senteces, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n","encoder_input_data = np.zeros(shape=(len(input_texts), max_decoder_seq_length), dtype=np.float32)\n","\n","# decoder_input_data is a 3D array of shape (num_sentences, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n","decoder_target_data = np.zeros(shape=(len(target_texts), max_decoder_seq_length), dtype=np.float32)\n"],"metadata":{"id":"FcEmF_tlrfYv","executionInfo":{"status":"ok","timestamp":1677734597442,"user_tz":300,"elapsed":109,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["OneHot Encoding"],"metadata":{"id":"7Xv-wjp8r3Dd"}},{"cell_type":"code","source":["for i, (input_sent, target_sent) in enumerate(zip(input_texts, target_texts)):\n","  for t, word in enumerate(input_sent.split()):\n","    encoder_input_data[i, t] =  input_token_index[word]\n","  encoder_input_data[i, t+1:] = input_token_index['pad'] # padding with spaces\n","  for t, word in enumerate(target_sent.split()):\n","    decoder_target_data[i, t] = output_token_index[word]\n","  decoder_target_data[i, t+1:] = output_token_index['pad']  # padding with spaces"],"metadata":{"id":"eUscK_onr4dy","executionInfo":{"status":"ok","timestamp":1677734602884,"user_tz":300,"elapsed":116,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["encoder_ohe = [one_hot(sent, n=ENCODER_VOCAB_SIZE, split=' ', lower=True) for sent in input_texts]\n","target_ohe = [one_hot(sent, n=TARGET_VOCAB_SIZE, split=' ', lower=True) for sent in target_texts]"],"metadata":{"id":"M-GzV-UWU0FI","executionInfo":{"status":"ok","timestamp":1677734603002,"user_tz":300,"elapsed":121,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["encoder_padded_docs = pad_sequences(encoder_ohe, padding='post', maxlen=max_decoder_seq_length)\n","target_padded_docs = pad_sequences(target_ohe, padding='post', maxlen=max_decoder_seq_length)"],"metadata":{"id":"vNXzh76oV2l8","executionInfo":{"status":"ok","timestamp":1677734603155,"user_tz":300,"elapsed":155,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["sent_a = input_texts[110]\n","sent_b = target_texts[110]\n","sent_a, sent_b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hY376kpiBV3","executionInfo":{"status":"ok","timestamp":1677734603156,"user_tz":300,"elapsed":7,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"29193517-193f-4df1-9732-6ee6cd84e5e9"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('i fell.', 'je suis tombé.')"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["encoder_input_data[110], decoder_target_data[110]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIsM7ybhir_Y","executionInfo":{"status":"ok","timestamp":1677734603156,"user_tz":300,"elapsed":5,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"132de48c-be2a-41e7-ad7c-3be2b94fb90e"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1193.,  835., 2726., 2726., 2726., 2726., 2726., 2726., 2726.,\n","        2726.], dtype=float32),\n"," array([2470., 4639., 4846., 5390., 5390., 5390., 5390., 5390., 5390.,\n","        5390.], dtype=float32))"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["tf.random.set_seed(10)\n","\n","num_layers = 6\n","embedding_dim = 4\n","num_heads = 4\n","fully_connected_dim = 8\n","input_vocab_size = 2877\n","target_vocab_size = 5632\n","max_positional_encoding_input = 10\n","max_positional_encoding_target = 11\n","\n","transformer = Transformer(num_layers, embedding_dim, num_heads, fully_connected_dim, \n","                          input_vocab_size, target_vocab_size, max_positional_encoding_input, \n","                          max_positional_encoding_target)\n","\n","enc_padding_mask = create_padding_mask(encoder_padded_docs)\n","dec_padding_mask = create_padding_mask(target_padded_docs)\n","\n","look_ahead_mask = create_look_ahead_mask(encoder_padded_docs.shape[1])\n","\n","translation, weights = transformer( encoder_padded_docs, target_padded_docs, True, enc_padding_mask, look_ahead_mask, dec_padding_mask )"],"metadata":{"id":"7YwiGbIQjrA6","executionInfo":{"status":"ok","timestamp":1677734608883,"user_tz":300,"elapsed":5599,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["# Reverse-lookup token index to decode sequences back to something readable.\n","reverse_input_word_index = dict((i, word) for word, i in input_token_index.items())\n","reverse_target_word_index = dict((i, word) for word, i in output_token_index.items())"],"metadata":{"id":"oTiwDE0epeDz","executionInfo":{"status":"ok","timestamp":1677734608883,"user_tz":300,"elapsed":7,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["for i, predicted_tf in enumerate(translation[:10]):\n","  print('----------------------------------------------------------------------------------------------------')\n","  french = ' '.join([reverse_target_word_index.get(np.argmax(word_pred)) for word_pred in predicted_tf.numpy()])\n","  print(f'English:{input_texts[i]}')\n","  print(f'French:{french}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3CrosTsqPlI","executionInfo":{"status":"ok","timestamp":1677734608884,"user_tz":300,"elapsed":6,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"6e3a153a-f87d-4606-fb12-6dd0d0183ee8"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","English:go.\n","French:relâchez-le relâchez-le relâchez-le attrapez relâchez-le attrapez relâchez-le tiens, tiens, conduisis.\n","----------------------------------------------------------------------------------------------------\n","English:go.\n","French:relâchez-le relâchez-le tiens, relâchez-le attrapez relâchez-le relâchez-le relâchez-le relâchez-le conduisis.\n","----------------------------------------------------------------------------------------------------\n","English:go.\n","French:relâchez-le relâchez-le relâchez-le relâchez-le attrapez relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le\n","----------------------------------------------------------------------------------------------------\n","English:go.\n","French:relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le\n","----------------------------------------------------------------------------------------------------\n","English:hi.\n","French:relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le redemandez ouvre. tiens, tiens,\n","----------------------------------------------------------------------------------------------------\n","English:hi.\n","French:ouvre. relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le relâchez-le attrapez\n","----------------------------------------------------------------------------------------------------\n","English:run!\n","French:relâchez-le relâchez-le relâchez-le avions attrapez relâchez-le relâchez-le relâchez-le relâchez-le conduisis.\n","----------------------------------------------------------------------------------------------------\n","English:run!\n","French:revoilà. accélère. voiles. attrapez revoilà. attrapez ouvre. accélère. attrapez attrapez\n","----------------------------------------------------------------------------------------------------\n","English:run!\n","French:revoilà. voiles. tiens, attrapez attrapez voiles. attrapez relâchez-le relâchez-le attrapez\n","----------------------------------------------------------------------------------------------------\n","English:run!\n","French:apeuré. revoilà. relâchez-le accélère. accélère. voiles. relâchez-le attrapez revoilà. relâchez-le\n"]}]},{"cell_type":"markdown","source":["<a name='7'></a> \n","## 6 - References\n","\n","Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n","\n","\n","Francois Chollet. \"A ten-minute introduction to sequence-to-sequence learning in Keras\". Keras Blog, 14 September 2016, https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html.\n","\n","\n","\n","\n","Jalammar, J. (2018, August 24). The Illustrated Transformer. Retrieved from http://jalammar.github.io/illustrated-transformer/\n","\n","\n","\n"],"metadata":{"id":"HG_PEigXvqhN"}}]}