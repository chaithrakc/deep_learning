{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1J4ivgXRDuiR9dkTDj82v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Transformer Network"],"metadata":{"id":"Lqys83YatfzL"}},{"cell_type":"markdown","source":["The research paper \"Attention Is All You Need\" published in 2017 introduced Transformer neural networks, which has brought about a revolutionary change in the field of NLP. The key innovation of the Transformer is its **`attention mechanism`**, which allows it to process input sequences in parallel and capture long-range dependencies more effectively.\n","\n","\n","**Link to Research Paper:** https://arxiv.org/abs/1706.03762\n","\n"],"metadata":{"id":"DHVuC6uYs6xt"}},{"cell_type":"markdown","source":["**Why Trasnformer?**\n","\n","Key things that makes transformer better than RNN and LSTM:\n","- Parellel processing of the inputs which can make the training process lot faster\n","- Better contextual representation of the input because of multi-head self attentions\n","\n","\n","**When to use Transformer?**\n","\n","Transformers are best suited for tasks where context and long-range dependencies are important. Their attention mechanisms allow them to capture these features more effectively.\n","\n","Some of the common NLP tasks that are well-suited for transformers include:\n","- Machine Translation\n","- Text Summarization\n","- Question Answering\n","- Named Entity Recognition etc.,\n"],"metadata":{"id":"Ihi_5P8lzsDS"}},{"cell_type":"markdown","source":["# Exploring Transformer Architecture through English to French Machine Translation\n","\n"],"metadata":{"id":"7Xw0Eb2r1Pvd"}},{"cell_type":"markdown","source":["## Table of Contents\n","\n","- [Python Libraries](#0)\n","- [1 - Positional Encoding](#1)\n","- [2 - Masking](#2)\n","    - [2.1 - Padding Mask](#2-1)\n","    - [2.2 - Look-ahead Mask](#2-2)\n","- [3 - Encoder](#4)\n","    - [4.1 Encoder Layer](#4-1)\n","    - [4.2 - Full Encoder](#4-2)\n","- [4 - Decoder](#5)\n","    - [5.1 - Decoder Layer](#5-1)\n","    - [5.2 - Full Decoder](#5-2)\n","- [5 - Transformer](#6)\n","- [6 - References](#7)"],"metadata":{"id":"WZzBYuhNuhlh"}},{"cell_type":"markdown","source":["<a name='0'></a>\n","## Python Libraries\n","\n","Loading all the required python packages."],"metadata":{"id":"hSGwUOqSutYo"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"9FjVj0jHs1p-","executionInfo":{"status":"ok","timestamp":1677979271633,"user_tz":300,"elapsed":8907,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"]},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Positional Encoding\n","\n","Positional encoding to the input sequence is a critical step in the Transformer architecture, as it enables the model to understand the sequential order of the tokens in the input sequence.\n","\n","The formula for calculating the positional encoding is as follows:\n","\n","$${PE}{(pos,k)} = sin(pos/10000^{2i/dmodel})$$\n","\n","$${PE}{(pos,k+1)} = cos(pos/10000^{2i/dmodel})$$\n","\n","- $pos$ is the position of the token in the sequence\n","- $k = 2i$ is the index of the the each dimension in positional encoding. So, $i= k//2$\n","- $dmodel$ is the dimension of the embedding vector.\n","\n","if $k=[0,1,2,3,4,5]$ indices of postional embedding, then $i=[0,0,1,1,2,2]$"],"metadata":{"id":"PnoR1txKwiCs"}},{"cell_type":"code","source":["def get_angles(pos, k, d_model:int):\n","  \"\"\"\n","  Arguments:\n","  pos -- (an array of shape (position, 1) representing the positions in the sequence\n","  k -- k (an array of shape (1, d_model) representing the indices of the embedding dimensions\n","  d_model -- integer representing the dimensionality of the model\n","  \n","  Returns:\n","  angles -- an array of shape (position, d_model) representing the angles for the positional encoding.\n","  \"\"\"\n","  i = k//2\n","  angles = pos/np.power(10_000, 2*i/d_model)\n","  return angles"],"metadata":{"id":"pNUKgkkJyHBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position:int, d_model:int ):\n","  \"\"\"\n","  Arguments:\n","  position: an integer indicating the maximum sequence length\n","  d_model: an integer indicating the dimensionality of the model\n","  \n","  Returns:\n","  encoding -- a 3D tensor of shape (1, position, d_model) representing the positional encoding for a sequence of length position\n","  \"\"\"\n","  angle_radians = get_angles(np.arange(position)[: , np.newaxis],\n","                             np.arange(d_model)[np.newaxis, :],\n","                             d_model)\n","  # apply sin to even indices in the array; 2i\n","  angle_radians[:, 0::2] = np.sin(angle_radians[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_radians[:,1:2] = np.cos(angle_radians[:, 1:2])\n","\n","  pos_encoding = angle_radians[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"lOqaDq8NrRx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["positional_encoding(4, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0h2LoamNmmb","executionInfo":{"status":"ok","timestamp":1677969287159,"user_tz":300,"elapsed":165,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"31edf9d9-3d99-48f3-8571-fa08e82168eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=\n","array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n","          0.0000000e+00],\n","        [ 8.4147096e-01,  5.4030228e-01,  2.5116222e-02,  2.5118865e-02,\n","          6.3095731e-04],\n","        [ 9.0929741e-01, -4.1614684e-01,  5.0216600e-02,  5.0237730e-02,\n","          1.2619144e-03],\n","        [ 1.4112000e-01, -9.8999250e-01,  7.5285293e-02,  7.5356595e-02,\n","          1.8928709e-03]]], dtype=float32)>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["<a name='2'></a>\n","## 2 - Masking\n","\n","There are two types of masks used when building Transformer network: *padding mask* and *look-ahead mask*. \n","\n","<a name='2-1'></a>\n","### 2.1 - Padding Mask\n","\n","It is important to feed sequences of uniform length to the transformer. We can pad the sequences with zeros, and truncate sequences that exceed maximum length of the model."],"metadata":{"id":"_GWLf9Tw6YyM"}},{"cell_type":"code","source":["def create_padding_mask(seq):\n","  \"\"\"\n","    Creates a mask tensor representing the padding positions in the input sequence.\n","    \n","    Arguments:\n","    seq -- a tensor of shape (batch_size, seq_len)\n","\n","    Returns:\n","    mask -- a tensor of shape (batch_size, 1, seq_len), where each position is 0 if the corresponding position in\n","    the input sequence is a padding position, and 1 otherwise.\n","  \"\"\"\n","  mask = 1 - tf.cast(tf.math.equal(seq, 0),dtype=tf.float32)\n","\n","  # reshaping mask so that it has an additional dimension, \n","  # which will be needed when applying the mask in the self-attention mechanism of the Transformer model. \n","  return mask[:, tf.newaxis, :]\n"],"metadata":{"id":"GvwNpZ8j7Put"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = tf.constant([[7., 6., 1., 0., 0.], \n","                 [1., 2., 3., 0., 0.], \n","                 [4., 5., 0., 0., 0.]])\n","print(create_padding_mask(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3n-44roPIY2p","executionInfo":{"status":"ok","timestamp":1677969287307,"user_tz":300,"elapsed":8,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"39c803fb-229d-43c4-8081-54fa91e84ad1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[1. 1. 1. 0. 0.]]\n","\n"," [[1. 1. 1. 0. 0.]]\n","\n"," [[1. 1. 0. 0. 0.]]], shape=(3, 1, 5), dtype=float32)\n"]}]},{"cell_type":"code","source":["print(tf.keras.activations.softmax(x)) # softmax without padding\n","print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9)) #softmax with padding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7dXWg-Amq-G","executionInfo":{"status":"ok","timestamp":1677969287436,"user_tz":300,"elapsed":6,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"867fd8a3-47c3-4b66-d0af-36a6c7030d75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[7.2876644e-01 2.6809821e-01 1.8064314e-03 6.6454901e-04 6.6454901e-04]\n"," [8.4437378e-02 2.2952460e-01 6.2391251e-01 3.1062774e-02 3.1062774e-02]\n"," [2.6502505e-01 7.2041273e-01 4.8541026e-03 4.8541026e-03 4.8541026e-03]], shape=(3, 5), dtype=float32)\n","tf.Tensor(\n","[[[0.72973627 0.26845497 0.00180884 0.         0.        ]\n","  [0.09003057 0.24472848 0.66524094 0.         0.        ]\n","  [0.26762316 0.72747517 0.00490169 0.         0.        ]]\n","\n"," [[0.72973627 0.26845497 0.00180884 0.         0.        ]\n","  [0.09003057 0.24472848 0.66524094 0.         0.        ]\n","  [0.26762316 0.72747517 0.00490169 0.         0.        ]]\n","\n"," [[0.7310586  0.26894143 0.         0.         0.        ]\n","  [0.26894143 0.7310586  0.         0.         0.        ]\n","  [0.26894143 0.7310586  0.         0.         0.        ]]], shape=(3, 3, 5), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["<a name='2-2'></a>\n","### 2.2 - Look-ahead Mask\n"],"metadata":{"id":"YJOvyBOs7NOO"}},{"cell_type":"code","source":["def create_look_ahead_mask(sequence_length):\n","  mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n","  return mask "],"metadata":{"id":"LnT4HTMl7QYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_look_ahead_mask(4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQP3dI8MIWZk","executionInfo":{"status":"ok","timestamp":1677969287554,"user_tz":300,"elapsed":122,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"c535febc-9417-4a47-f4fb-371b2e51dc90"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=\n","array([[[1., 0., 0., 0.],\n","        [1., 1., 0., 0.],\n","        [1., 1., 1., 0.],\n","        [1., 1., 1., 1.]]], dtype=float32)>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["<a name='4'></a>\n","## 3 - Encoder\n","\n","Encoder contains - multi-head self attention layers and feed forward neural network that is independently applied to every position."],"metadata":{"id":"PO1-1ErWOC0k"}},{"cell_type":"code","source":["def FeedForward(embedding_dim, full_connected_dim):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(full_connected_dim, activation='relu'),\n","      tf.keras.layers.Dense(embedding_dim)\n","  ])"],"metadata":{"id":"Uvx9UkMAOIYj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, full_connected_dim,dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, full_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon = layernorm_eps )\n","    self.layer_norm2 = LayerNormalization(epsilon = layernorm_eps)\n","    self.drop_out = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    self_mha_output = self.mha(x,x,x,mask) # if query, key, value are same, then self-attenstion will be computed\n","    out1 = self.layer_norm1(x + self_mha_output)\n","    ffn_output = self.ffnn(out1)\n","    ffn_output = self.drop_out(ffn_output, training=training)\n","    encoder_layer_out = self.layer_norm2(out1 + ffn_output)\n","    return encoder_layer_out\n"],"metadata":{"id":"MxBFDD4pQ89U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_encoders, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","               max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_encoders\n","    self.embedding = Embedding(input_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.enc_layers = [EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate, layernorm_eps) for _ in range(num_encoders)]\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    seq_len = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # scaling: This is done to prevent the dot product operation in the self-attention mechanism from getting too large or too small\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x # tensor of shape (batch_size, input_seq_len, embedding_dim)"],"metadata":{"id":"k_6gxc_2pF7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name='5'></a>\n","## 4 - Decoder\n"],"metadata":{"id":"myl_skWyOIqn"}},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.masked_mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, fully_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm2 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm3 = LayerNormalization(epsilon=layernorm_eps)\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    mult_attn_out1, attn_weights_block1 = self.masked_mha(x, x, x, look_ahead_mask, return_attention_scores=True)\n","    Q1 = self.layer_norm1(mult_attn_out1 + x)\n","    mult_attn_out2, attn_weights_block2 = self.mha(Q1, enc_output, enc_output, padding_mask, return_attention_scores=True) \n","    mult_attn_out2 = self.layer_norm2(mult_attn_out2 + Q1)\n","    ffn_output = self.ffnn(mult_attn_out2)\n","    ffn_output = self.dropout(ffn_output, training = training)\n","    out3 = self.layer_norm3(ffn_output + mult_attn_out2)\n","    return out3, attn_weights_block1, attn_weights_block2\n","    "],"metadata":{"id":"nQFSLdQ-OM89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_decoders, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_decoders\n","    self.embedding = Embedding(target_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.dec_layers = [DecoderLayer(embedding_dim, num_heads, fully_connected_dim) for _ in range(num_decoders)]\n","    self.dropout = Dropout(dropout_rate)\n","\n","  def __call__(self,x, enc_output, training, look_ahead_mask, padding_mask):\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    x = self.embedding(x)\n","    #scaling\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","      attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n","    return x, attention_weights"],"metadata":{"id":"bX0w0MixtG7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name='6'></a> \n","## 5 - Transformer\n"],"metadata":{"id":"ISOTSziqONhq"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, \n","               max_pos_encoding_input, max_pos_encoding_target):\n","    super().__init__()\n","    self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_pos_encoding_input)\n","    self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding_target)\n","    self.final_layer = Dense(target_vocab_size, activation='softmax')\n","  \n","  def __call__(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","    enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n","    dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask )\n","    final_output = self.final_layer(dec_output)\n","    return final_output, attention_weights"],"metadata":{"id":"Cy6wxAFqOOUn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Machine Translation (English to French) using the above transformer"],"metadata":{"id":"OqexyS2prDmV"}},{"cell_type":"markdown","source":["Loading the dataset"],"metadata":{"id":"3edc0c4F1mo6"}},{"cell_type":"code","source":["!!curl -O http://www.manythings.org/anki/fra-eng.zip\n","!!unzip fra-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQITwmyJOkUr","executionInfo":{"status":"ok","timestamp":1677969548553,"user_tz":300,"elapsed":9727,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"06f23b41-0623-44a4-b437-7e46c09d040c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Archive:  fra-eng.zip',\n"," 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y',\n"," '  inflating: _about.txt              ',\n"," 'replace fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y',\n"," '  inflating: fra.txt                 ']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["data_path = \"fra.txt\"\n","batch_size= 10_000\n","input_texts = []\n","target_texts = []\n","input_vocab = set()\n","target_vocab = set()\n","\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","    \n","for line in lines[: min(batch_size, len(lines) - 1)]:\n","  line = line.lower()\n","  input_text, target_text, _ = line.split(\"\\t\")\n","  input_texts.append(input_text)\n","  target_texts.append(target_text)\n","\n","  for word in input_text.lower().split():\n","    if word not in input_vocab:\n","      input_vocab.add(word)\n","\n","  for word in target_text.split():\n","    if word not in target_vocab:\n","      target_vocab.add(word)\n"],"metadata":{"id":"GpnNbdqlOqbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_vocab_sroted = sorted(list(input_vocab))\n","target_vocab_sroted = sorted(list(target_vocab))\n","\n","input_vocab_sroted.append('pad')\n","target_vocab_sroted.append('pad')\n","\n","ENCODER_VOCAB_SIZE = len(input_vocab_sroted)\n","TARGET_VOCAB_SIZE = len(target_vocab_sroted)\n","\n","# needed for padding\n","max_encoder_seq_length = max([len(sentence.split()) for sentence in input_texts])\n","max_decoder_seq_length = max([len(sentence.split()) for sentence in target_texts])\n","\n","print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", ENCODER_VOCAB_SIZE)\n","print(\"Number of unique target tokens:\", TARGET_VOCAB_SIZE)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP4E7qu8PFcA","executionInfo":{"status":"ok","timestamp":1677969291327,"user_tz":300,"elapsed":5,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"c0b18911-3d30-4a98-8f71-5d9c8ead00c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 10000\n","Number of unique input tokens: 2727\n","Number of unique target tokens: 5391\n","Max sequence length for inputs: 4\n","Max sequence length for outputs: 10\n"]}]},{"cell_type":"code","source":["# adding index to each character in the sorted list\n","input_token_index = dict([(word, i) for i, word in enumerate(input_vocab_sroted)])\n","output_token_index = dict([(word, i) for i, word in enumerate(target_vocab_sroted)])"],"metadata":{"id":"PbhivCQZoUeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# intializing one hot encode vectors\n","\n","# encoder_input_data is a 3D array of shape (num_senteces, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n","encoder_input_data = np.zeros(shape=(len(input_texts), max_decoder_seq_length), dtype=np.float32)\n","\n","# decoder_input_data is a 3D array of shape (num_sentences, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n","decoder_target_data = np.zeros(shape=(len(target_texts), max_decoder_seq_length), dtype=np.float32)\n"],"metadata":{"id":"FcEmF_tlrfYv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OneHot Encoding"],"metadata":{"id":"7Xv-wjp8r3Dd"}},{"cell_type":"code","source":["for i, (input_sent, target_sent) in enumerate(zip(input_texts, target_texts)):\n","  for t, word in enumerate(input_sent.split()):\n","    encoder_input_data[i, t] =  input_token_index[word]\n","  encoder_input_data[i, t+1:] = input_token_index['pad'] # padding with spaces\n","  for t, word in enumerate(target_sent.split()):\n","    decoder_target_data[i, t] = output_token_index[word]\n","  decoder_target_data[i, t+1:] = output_token_index['pad']  # padding with spaces"],"metadata":{"id":"eUscK_onr4dy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training the Transformer"],"metadata":{"id":"qOELwf2q1tlU"}},{"cell_type":"code","source":["tf.random.set_seed(10)\n","\n","num_layers = 6\n","embedding_dim = 4\n","num_heads = 4\n","fully_connected_dim = 8\n","input_vocab_size = ENCODER_VOCAB_SIZE\n","target_vocab_size = TARGET_VOCAB_SIZE\n","max_positional_encoding_input = 10\n","max_positional_encoding_target = 11\n","\n","transformer = Transformer(num_layers, embedding_dim, num_heads, fully_connected_dim, \n","                          input_vocab_size, target_vocab_size, max_positional_encoding_input, \n","                          max_positional_encoding_target)\n","\n","enc_padding_mask = create_padding_mask(encoder_input_data)\n","dec_padding_mask = create_padding_mask(decoder_target_data)\n","\n","look_ahead_mask = create_look_ahead_mask(max_positional_encoding_input)\n","\n","translation, weights = transformer( encoder_input_data, decoder_target_data, True, enc_padding_mask, look_ahead_mask, dec_padding_mask )"],"metadata":{"id":"7YwiGbIQjrA6","executionInfo":{"status":"ok","timestamp":1677970841665,"user_tz":300,"elapsed":11186,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n","\n","def compute_accuracy(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracy_ = tf.keras.metrics.sparse_categorical_accuracy(real, pred)\n","    mask = tf.cast(mask, dtype=accuracy_.dtype)\n","    accuracy_ *= mask\n","    return tf.reduce_sum(accuracy_) / tf.reduce_sum(mask)\n"],"metadata":{"id":"wfGKIICCUrx5","executionInfo":{"status":"ok","timestamp":1677978804399,"user_tz":300,"elapsed":102,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def train_step(model, input_sentence, output_sentence, target_sentence, enc_padding_mask, look_ahead_mask, dec_padding_mask, optimizer, loss_function):\n","    with tf.GradientTape() as tape:\n","        predictions, _ = model(input_sentence, output_sentence, True, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n","        loss = loss_function(target_sentence, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    return loss\n"],"metadata":{"id":"yU__gd6aTZpe","executionInfo":{"status":"ok","timestamp":1677978811198,"user_tz":300,"elapsed":98,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def fit(model, input_dataset, output_dataset, trainable, enc_padding_mask, look_ahead_mask, dec_padding_mask, optimizer, loss_function, epochs):\n","    train_losses = []\n","    train_accs = []\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        for batch, (input_sentence, output_sentence, target_sentence) in enumerate(train_dataset):\n","            enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(input_sentence, output_sentence)\n","            loss = train_step(model, input_sentence, output_sentence, target_sentence, enc_padding_mask, combined_mask, dec_padding_mask, optimizer, loss_function)\n","            epoch_loss += loss\n","            epoch_acc += compute_accuracy(target_sentence, model(input_sentence, output_sentence, False, enc_padding_mask, combined_mask, dec_padding_mask)[0])\n","        train_losses.append(epoch_loss / (batch + 1))\n","        train_accs.append(epoch_acc / (batch + 1))\n","        print(f'Epoch {epoch + 1}, Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accs[-1]:.4f}')\n","    return train_losses, train_accs\n"],"metadata":{"id":"HjnVGj0vTiYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n","\n","train_losses, train_accs = fit(transformer,encoder_input_data, decoder_target_data, True, \n","                               enc_padding_mask, look_ahead_mask, dec_padding_mask, optimizer, loss_function, metrics )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"lEVkGSfwTlCI","executionInfo":{"status":"error","timestamp":1677979250058,"user_tz":300,"elapsed":109,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"6be8a77c-4496-401d-c13b-57465bdb48be"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4c1edc96a03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_losses, train_accs = fit(transformer,encoder_input_data, decoder_target_data, True, \n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2JYTwC0FRyot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reverse-lookup token index to decode sequences back to something readable.\n","reverse_input_word_index = dict((i, word) for word, i in input_token_index.items())\n","reverse_target_word_index = dict((i, word) for word, i in output_token_index.items())"],"metadata":{"id":"oTiwDE0epeDz","executionInfo":{"status":"ok","timestamp":1677970849172,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["for i, predicted_tf in enumerate(translation[:10]):\n","  print('-------------')\n","  french_pred = ' '.join([reverse_target_word_index.get(np.argmax(pred)) for pred in predicted_tf.numpy()])\n","  print(f'English:{input_texts[i]}')\n","  print(f'French:{target_texts[i]}')\n","  print(f'Predicted French:{french_pred}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3CrosTsqPlI","executionInfo":{"status":"ok","timestamp":1677970857150,"user_tz":300,"elapsed":99,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"13cbbb51-bd91-4783-a8e0-2e4ed2634639"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------\n","English:go.\n","French:va !\n","Predicted French:pourrais-tu moi. motivé. beignet. gavée. gavée. gavée. gavée. moi. trépasser\n","-------------\n","English:go.\n","French:marche.\n","Predicted French:pourrais-tu cessez normales. sucré. pourrais-tu pourrais-tu pourrais-tu gavée. moi. sucré.\n","-------------\n","English:go.\n","French:en route !\n","Predicted French:pourrais-tu mortel. compte. sucré. sucré. pourrais-tu pourrais-tu matériel motivé. pourrais-tu\n","-------------\n","English:go.\n","French:bouge !\n","Predicted French:pourrais-tu demande-lui. sucré. trépasser pourrais-tu synchronisés. gavée. préparé normales. renonçons.\n","-------------\n","English:hi.\n","French:salut !\n","Predicted French:pourrais-tu préparé normales. pourrais-tu pourrais-tu gavée. pourrais-tu pourrais-tu renonçons. sucré.\n","-------------\n","English:hi.\n","French:salut.\n","Predicted French:pourrais-tu matériel normales. compte. pourrais-tu pourrais-tu gavée. pourrais-tu normales. regarderai.\n","-------------\n","English:run!\n","French:cours !\n","Predicted French:pourrais-tu rappelle. sucré. regarderai. pourrais-tu gavée. pourrais-tu rappelle. renonçons. sucré.\n","-------------\n","English:run!\n","French:courez !\n","Predicted French:pourrais-tu préparé sucré. sucré. sucré. gavée. corps. gavée. normales. sucré.\n","-------------\n","English:run!\n","French:prenez vos jambes à vos cous !\n","Predicted French:pourrais-tu cessez motivé. sucré. revenu. pourrais-tu pourrais-tu demande-lui. moi. sucré.\n","-------------\n","English:run!\n","French:file !\n","Predicted French:regarderai. pourvu moi. motivé. sucré. gavée. pourrais-tu gavée. sucré. pourrais-tu\n"]}]},{"cell_type":"markdown","source":["<a name='7'></a> \n","## 6 - References\n","\n","Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n","\n","\n","Francois Chollet. \"A ten-minute introduction to sequence-to-sequence learning in Keras\". Keras Blog, 14 September 2016, https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html.\n","\n","\n","\n","\n","Jalammar, J. (2018, August 24). The Illustrated Transformer. Retrieved from http://jalammar.github.io/illustrated-transformer/\n","\n","\n","\n"],"metadata":{"id":"HG_PEigXvqhN"}},{"cell_type":"markdown","source":["# Comparing with LSTM and RNN\n","\n","- The main idea behind Transformers is to replace recursive approach in RNNs by self-attnetion mechanism.\n","\n","- Self-attention mechanism allows the model to encode the input sequence by including the context by attending to different parts of the input sequence (via Query, Key, and Value tensors)\n","\n","- For example, if we want to translate the sentence \"I am a student\" to French, we would feed the input sequence one word at a time and update the hidden state at each time step. The final hidden state would be used to generate the output \"je suis étudiant.\"\n","\n","- In transformers, we would compute self-attention scores between all the words in the input sequence, and use these scores to compute a weighted sum of the input sequence to obtain a representation for each output element. This allows the model to consider all the words in the input sequence at once, and to capture long-range dependencies between them.\n","\n","\n","Advantages of Transformers:\n","- Parellel processing of the inputs; faster training\n","- Does not suffer from vanishing gradient or exploding gradient problem; makes it easy to train sequences"],"metadata":{"id":"QLsuXiKUxgfW"}}]}