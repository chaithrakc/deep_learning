{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxYndc60tYX4vUQVNiThwZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Transformer Network"],"metadata":{"id":"Lqys83YatfzL"}},{"cell_type":"markdown","source":["The research paper \"Attention Is All You Need\" published in 2017 introduced Transformer neural networks, which has brought about a revolutionary change in the field of NLP. The key innovation of the Transformer is its **`attention mechanism`**, which allows it to process input sequences in parallel and capture long-range dependencies more effectively.\n","\n","\n","**Link to Research Paper:** https://arxiv.org/abs/1706.03762\n","\n"],"metadata":{"id":"DHVuC6uYs6xt"}},{"cell_type":"markdown","source":["**Why Trasnformer?**\n","\n","Key things that makes transformer better than RNN and LSTM:\n","- Parellel processing of the inputs which can make the training process lot faster\n","- Better contextual representation of the input because of multi-head self attentions\n","\n","\n","**When to use Transformer?**\n","\n","Transformers are best suited for tasks where context and long-range dependencies are important. Their attention mechanisms allow them to capture these features more effectively.\n","\n","Some of the common NLP tasks that are well-suited for transformers include:\n","- Machine Translation\n","- Text Summarization\n","- Question Answering\n","- Named Entity Recognition etc.,\n"],"metadata":{"id":"Ihi_5P8lzsDS"}},{"cell_type":"markdown","source":["# Exploring Transformer Architecture through English to French Machine Translation\n","\n"],"metadata":{"id":"7Xw0Eb2r1Pvd"}},{"cell_type":"markdown","source":["## Table of Contents\n","\n","- [Python Libraries](#0)\n","- [1 - Positional Encoding](#1)\n","- [2 - Masking](#2)\n","    - [2.1 - Padding Mask](#2-1)\n","    - [2.2 - Look-ahead Mask](#2-2)\n","- [3 - Encoder](#4)\n","    - [4.1 Encoder Layer](#4-1)\n","    - [4.2 - Full Encoder](#4-2)\n","- [4 - Decoder](#5)\n","    - [5.1 - Decoder Layer](#5-1)\n","    - [5.2 - Full Decoder](#5-2)\n","- [5 - Transformer](#6)\n","- [6 - References](#7)"],"metadata":{"id":"WZzBYuhNuhlh"}},{"cell_type":"markdown","source":["<a name='0'></a>\n","## Python Libraries\n","\n","Loading all the required python packages."],"metadata":{"id":"hSGwUOqSutYo"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dgan9SOhBDbL","executionInfo":{"status":"ok","timestamp":1677975553389,"user_tz":300,"elapsed":5783,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"ca2dc933-5d84-416f-9786-7ff912cdae7e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n","Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.31.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","execution_count":68,"metadata":{"id":"9FjVj0jHs1p-","executionInfo":{"status":"ok","timestamp":1677982061634,"user_tz":300,"elapsed":115,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow import GradientTape, train, function\n","from tensorflow.keras.metrics import Mean\n","from time import time\n","from pickle import dump\n","from pickle import load\n"]},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Positional Encoding\n","\n","Positional encoding to the input sequence is a critical step in the Transformer architecture, as it enables the model to understand the sequential order of the tokens in the input sequence.\n","\n","The formula for calculating the positional encoding is as follows:\n","\n","$${PE}{(pos,k)} = sin(pos/10000^{2i/dmodel})$$\n","\n","$${PE}{(pos,k+1)} = cos(pos/10000^{2i/dmodel})$$\n","\n","- $pos$ is the position of the token in the sequence\n","- $k = 2i$ is the index of the the each dimension in positional encoding. So, $i= k//2$\n","- $dmodel$ is the dimension of the embedding vector.\n","\n","if $k=[0,1,2,3,4,5]$ indices of postional embedding, then $i=[0,0,1,1,2,2]$"],"metadata":{"id":"PnoR1txKwiCs"}},{"cell_type":"code","source":["def get_angles(pos, k, d_model:int):\n","  \"\"\"\n","  Arguments:\n","  pos -- (an array of shape (position, 1) representing the positions in the sequence\n","  k -- k (an array of shape (1, d_model) representing the indices of the embedding dimensions\n","  d_model -- integer representing the dimensionality of the model\n","  \n","  Returns:\n","  angles -- an array of shape (position, d_model) representing the angles for the positional encoding.\n","  \"\"\"\n","  i = k//2\n","  angles = pos/np.power(10_000, 2*i/d_model)\n","  return angles"],"metadata":{"id":"pNUKgkkJyHBU","executionInfo":{"status":"ok","timestamp":1677975558865,"user_tz":300,"elapsed":5,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position:int, d_model:int ):\n","  \"\"\"\n","  Arguments:\n","  position: an integer indicating the maximum sequence length\n","  d_model: an integer indicating the dimensionality of the model\n","  \n","  Returns:\n","  encoding -- a 3D tensor of shape (1, position, d_model) representing the positional encoding for a sequence of length position\n","  \"\"\"\n","  angle_radians = get_angles(np.arange(position)[: , np.newaxis],\n","                             np.arange(d_model)[np.newaxis, :],\n","                             d_model)\n","  # apply sin to even indices in the array; 2i\n","  angle_radians[:, 0::2] = np.sin(angle_radians[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_radians[:,1:2] = np.cos(angle_radians[:, 1:2])\n","\n","  pos_encoding = angle_radians[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"lOqaDq8NrRx-","executionInfo":{"status":"ok","timestamp":1677975563128,"user_tz":300,"elapsed":4267,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["positional_encoding(4, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0h2LoamNmmb","executionInfo":{"status":"ok","timestamp":1677975563329,"user_tz":300,"elapsed":96,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"09d55361-9235-4555-9713-3aa4a6084d0c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=\n","array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n","          0.0000000e+00],\n","        [ 8.4147096e-01,  5.4030228e-01,  2.5116222e-02,  2.5118865e-02,\n","          6.3095731e-04],\n","        [ 9.0929741e-01, -4.1614684e-01,  5.0216600e-02,  5.0237730e-02,\n","          1.2619144e-03],\n","        [ 1.4112000e-01, -9.8999250e-01,  7.5285293e-02,  7.5356595e-02,\n","          1.8928709e-03]]], dtype=float32)>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["<a name='2'></a>\n","## 2 - Masking\n","\n","There are two types of masks used when building Transformer network: *padding mask* and *look-ahead mask*. \n","\n","<a name='2-1'></a>\n","### 2.1 - Padding Mask\n","\n","It is important to feed sequences of uniform length to the transformer. We can pad the sequences with zeros, and truncate sequences that exceed maximum length of the model."],"metadata":{"id":"_GWLf9Tw6YyM"}},{"cell_type":"code","source":["def create_padding_mask(seq):\n","  \"\"\"\n","    Creates a mask tensor representing the padding positions in the input sequence.\n","    \n","    Arguments:\n","    seq -- a tensor of shape (batch_size, seq_len)\n","\n","    Returns:\n","    mask -- a tensor of shape (batch_size, 1, seq_len), where each position is 0 if the corresponding position in\n","    the input sequence is a padding position, and 1 otherwise.\n","  \"\"\"\n","  mask = 1 - tf.cast(tf.math.equal(seq, 0),dtype=tf.float32)\n","\n","  # reshaping mask so that it has an additional dimension, \n","  # which will be needed when applying the mask in the self-attention mechanism of the Transformer model. \n","  return mask[:, tf.newaxis, :]\n"],"metadata":{"id":"GvwNpZ8j7Put","executionInfo":{"status":"ok","timestamp":1677975563329,"user_tz":300,"elapsed":2,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["x = tf.constant([[7., 6., 1., 0., 0.], \n","                 [1., 2., 3., 0., 0.], \n","                 [4., 5., 0., 0., 0.]])\n","print(create_padding_mask(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3n-44roPIY2p","executionInfo":{"status":"ok","timestamp":1677975563483,"user_tz":300,"elapsed":156,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"4a50c4b4-5f55-43ef-ea04-77f47a52454e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[1. 1. 1. 0. 0.]]\n","\n"," [[1. 1. 1. 0. 0.]]\n","\n"," [[1. 1. 0. 0. 0.]]], shape=(3, 1, 5), dtype=float32)\n"]}]},{"cell_type":"code","source":["print(tf.keras.activations.softmax(x)) # softmax without padding\n","print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9)) #softmax with padding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7dXWg-Amq-G","executionInfo":{"status":"ok","timestamp":1677975563729,"user_tz":300,"elapsed":156,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"d975a598-ca29-4638-d29a-b57a9d871a52"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[7.2876632e-01 2.6809818e-01 1.8064311e-03 6.6454883e-04 6.6454883e-04]\n"," [8.4437370e-02 2.2952460e-01 6.2391245e-01 3.1062772e-02 3.1062772e-02]\n"," [2.6502502e-01 7.2041267e-01 4.8541022e-03 4.8541022e-03 4.8541022e-03]], shape=(3, 5), dtype=float32)\n","tf.Tensor(\n","[[[0.7297362  0.26845497 0.00180884 0.         0.        ]\n","  [0.09003057 0.24472848 0.6652409  0.         0.        ]\n","  [0.26762316 0.7274751  0.00490169 0.         0.        ]]\n","\n"," [[0.7297362  0.26845497 0.00180884 0.         0.        ]\n","  [0.09003057 0.24472848 0.6652409  0.         0.        ]\n","  [0.26762316 0.7274751  0.00490169 0.         0.        ]]\n","\n"," [[0.73105854 0.26894143 0.         0.         0.        ]\n","  [0.26894143 0.73105854 0.         0.         0.        ]\n","  [0.26894143 0.73105854 0.         0.         0.        ]]], shape=(3, 3, 5), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["<a name='2-2'></a>\n","### 2.2 - Look-ahead Mask\n"],"metadata":{"id":"YJOvyBOs7NOO"}},{"cell_type":"code","source":["def create_look_ahead_mask(sequence_length):\n","  mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n","  return mask "],"metadata":{"id":"LnT4HTMl7QYk","executionInfo":{"status":"ok","timestamp":1677975563730,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["create_look_ahead_mask(4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQP3dI8MIWZk","executionInfo":{"status":"ok","timestamp":1677975563730,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"a3b8bea6-6102-40cb-ba11-eec5fab768fb"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=\n","array([[[1., 0., 0., 0.],\n","        [1., 1., 0., 0.],\n","        [1., 1., 1., 0.],\n","        [1., 1., 1., 1.]]], dtype=float32)>"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["<a name='4'></a>\n","## 3 - Encoder\n","\n","Encoder contains - multi-head self attention layers and feed forward neural network that is independently applied to every position."],"metadata":{"id":"PO1-1ErWOC0k"}},{"cell_type":"code","source":["def FeedForward(embedding_dim, full_connected_dim):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(full_connected_dim, activation='relu'),\n","      tf.keras.layers.Dense(embedding_dim)\n","  ])"],"metadata":{"id":"Uvx9UkMAOIYj","executionInfo":{"status":"ok","timestamp":1677975563842,"user_tz":300,"elapsed":115,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, full_connected_dim,dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, full_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon = layernorm_eps )\n","    self.layer_norm2 = LayerNormalization(epsilon = layernorm_eps)\n","    self.drop_out = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    self_mha_output = self.mha(x,x,x,mask) # if query, key, value are same, then self-attenstion will be computed\n","    out1 = self.layer_norm1(x + self_mha_output)\n","    ffn_output = self.ffnn(out1)\n","    ffn_output = self.drop_out(ffn_output, training=training)\n","    encoder_layer_out = self.layer_norm2(out1 + ffn_output)\n","    return encoder_layer_out\n"],"metadata":{"id":"MxBFDD4pQ89U","executionInfo":{"status":"ok","timestamp":1677975563842,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_encoders, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","               max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_encoders\n","    self.embedding = Embedding(input_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.enc_layers = [EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate, layernorm_eps) for _ in range(num_encoders)]\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, training, mask):\n","    seq_len = tf.shape(x)[1]\n","    x = self.embedding(x)\n","    # scaling: This is done to prevent the dot product operation in the self-attention mechanism from getting too large or too small\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x # tensor of shape (batch_size, input_seq_len, embedding_dim)"],"metadata":{"id":"k_6gxc_2pF7g","executionInfo":{"status":"ok","timestamp":1677975563843,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["<a name='5'></a>\n","## 4 - Decoder\n"],"metadata":{"id":"myl_skWyOIqn"}},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n","    super().__init__()\n","    self.masked_mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.mha = MultiHeadAttention(num_heads, key_dim=embedding_dim, dropout=dropout_rate)\n","    self.ffnn = FeedForward(embedding_dim, fully_connected_dim)\n","    self.layer_norm1 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm2 = LayerNormalization(epsilon=layernorm_eps)\n","    self.layer_norm3 = LayerNormalization(epsilon=layernorm_eps)\n","    self.dropout = Dropout(dropout_rate)\n","  \n","  def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    mult_attn_out1, attn_weights_block1 = self.masked_mha(x, x, x, look_ahead_mask, return_attention_scores=True)\n","    Q1 = self.layer_norm1(mult_attn_out1 + x)\n","    mult_attn_out2, attn_weights_block2 = self.mha(Q1, enc_output, enc_output, padding_mask, return_attention_scores=True) \n","    mult_attn_out2 = self.layer_norm2(mult_attn_out2 + Q1)\n","    ffn_output = self.ffnn(mult_attn_out2)\n","    ffn_output = self.dropout(ffn_output, training = training)\n","    out3 = self.layer_norm3(ffn_output + mult_attn_out2)\n","    return out3, attn_weights_block1, attn_weights_block2\n","    "],"metadata":{"id":"nQFSLdQ-OM89","executionInfo":{"status":"ok","timestamp":1677975563843,"user_tz":300,"elapsed":4,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_decoders, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding, dropout_rate=0.1, layernorm_eps=1e-6 ):\n","    super().__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_layers = num_decoders\n","    self.embedding = Embedding(target_vocab_size, embedding_dim)\n","    self.pos_encoding = positional_encoding(max_pos_encoding, embedding_dim)\n","    self.dec_layers = [DecoderLayer(embedding_dim, num_heads, fully_connected_dim) for _ in range(num_decoders)]\n","    self.dropout = Dropout(dropout_rate)\n","\n","  def __call__(self,x, enc_output, training, look_ahead_mask, padding_mask):\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    x = self.embedding(x)\n","    #scaling\n","    x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    x = self.dropout(x, training = training)\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","      attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n","    return x, attention_weights"],"metadata":{"id":"bX0w0MixtG7e","executionInfo":{"status":"ok","timestamp":1677975563843,"user_tz":300,"elapsed":3,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["<a name='6'></a> \n","## 5 - Transformer\n"],"metadata":{"id":"ISOTSziqONhq"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, \n","               max_pos_encoding_input, max_pos_encoding_target):\n","    super().__init__()\n","    self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_pos_encoding_input)\n","    self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_pos_encoding_target)\n","    self.final_layer = Dense(target_vocab_size, activation='softmax')\n","\n","  def create_padding_mask(self, seq):\n","    mask = 1 - tf.cast(tf.math.equal(seq, 0),dtype=tf.float32)\n","    return mask[:, tf.newaxis, :]\n","  \n","  def create_look_ahead_mask(sequence_length):\n","    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n","    return mask \n","    \n","  def __call__(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","    enc_padding_mask = self.create_padding_mask(input_sentence)\n","    dec_padding_mask = self.create_padding_mask(output_sentence)\n","    look_ahead_mask = self.create_look_ahead_mask(output_sentence.shape[1])\n","    \n","    dec_in_padding_mask = self.padding_mask(decoder_input)\n","    dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n","\n","    enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n","    dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask )\n","    final_output = self.final_layer(dec_output)\n","    return final_output, attention_weights"],"metadata":{"id":"Cy6wxAFqOOUn","executionInfo":{"status":"ok","timestamp":1677981793809,"user_tz":300,"elapsed":130,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["Machine Translation (English to French) using the above transformer"],"metadata":{"id":"OqexyS2prDmV"}},{"cell_type":"markdown","source":["Loading the dataset"],"metadata":{"id":"3edc0c4F1mo6"}},{"cell_type":"code","source":["!!curl -O http://www.manythings.org/anki/hin-eng.zip\n","!!unzip hin-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQITwmyJOkUr","executionInfo":{"status":"ok","timestamp":1677975564855,"user_tz":300,"elapsed":924,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"31f45922-0847-4d1b-b6c2-1c0af7019d59"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Archive:  hin-eng.zip',\n"," '  inflating: hin.txt                 ',\n"," '  inflating: _about.txt              ']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["data_path = \"hin.txt\"\n","batch_size= 10_000\n","enc_seq_length = 30\n","dec_seq_length = 30\n","input_texts = []\n","target_texts = []\n","\n","\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","  lines = f.read().split(\"\\n\")\n","  for line in lines[: min(batch_size, len(lines) - 1)]:\n","    line = line.lower()\n","    input_text, target_text, _ = line.split(\"\\t\")\n","    input_texts.append('<sos> '+input_text+' <eos>')\n","    target_texts.append('<sos> ' + target_text + ' <eos>')"],"metadata":{"id":"GpnNbdqlOqbe","executionInfo":{"status":"ok","timestamp":1677975565016,"user_tz":300,"elapsed":163,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Text Vectorization"],"metadata":{"id":"0ENTsaloCPlI"}},{"cell_type":"code","source":["input_text_vectorizer = TextVectorization(max_tokens=10_000, output_mode='int')\n","input_tf_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(input_texts, dtype=tf.string))\n","input_text_vectorizer.adapt(input_tf_dataset)\n","input_tensors = input_tf_dataset.map(input_text_vectorizer)\n","input_tensors = pad_sequences(input_tensors, padding='post', maxlen=enc_seq_length)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1677975585928,"user_tz":300,"elapsed":20915,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"id":"4RJ9fZg0CWkM"},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["output_text_vectorizer = TextVectorization(max_tokens=10_000, output_mode='int')\n","output_tf_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(target_texts, dtype=tf.string))\n","output_text_vectorizer.adapt(output_tf_dataset)\n","output_tensors = output_tf_dataset.map(output_text_vectorizer)\n","output_tensors = pad_sequences(output_tensors, padding='post', maxlen=dec_seq_length)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1677975597077,"user_tz":300,"elapsed":11153,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"id":"in0u4X_iCWkM"},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["ENCODER_VOCAB_SIZE = input_text_vectorizer.vocabulary_size()\n","TARGET_VOCAB_SIZE = output_text_vectorizer.vocabulary_size()\n","\n","\n","print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", ENCODER_VOCAB_SIZE)\n","print(\"Number of unique target tokens:\", TARGET_VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP4E7qu8PFcA","executionInfo":{"status":"ok","timestamp":1677975597078,"user_tz":300,"elapsed":32,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"3c57c25f-cc69-4c96-e60f-5cb39e30acb8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 2980\n","Number of unique input tokens: 2410\n","Number of unique target tokens: 3067\n"]}]},{"cell_type":"markdown","source":["Training the Transformer"],"metadata":{"id":"qOELwf2q1tlU"}},{"cell_type":"code","source":["tf.random.set_seed(10)\n","\n","transformer = Transformer(num_layers=6, embedding_dim=4, num_heads=4, fully_connected_dim=8,\n","                          input_vocab_size=ENCODER_VOCAB_SIZE, target_vocab_size=TARGET_VOCAB_SIZE,\n","                          max_pos_encoding_input=enc_seq_length,\n","                          max_pos_encoding_target=dec_seq_length)\n","\n","\n","enc_padding_mask = create_padding_mask(input_tensors)\n","dec_padding_mask = create_padding_mask(output_tensors)\n","look_ahead_mask = create_look_ahead_mask(dec_seq_length)\n","\n","translation, weights = transformer( input_tensors, output_tensors, True, enc_padding_mask, look_ahead_mask, dec_padding_mask )"],"metadata":{"id":"7YwiGbIQjrA6","executionInfo":{"status":"ok","timestamp":1677975601780,"user_tz":300,"elapsed":4730,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)"],"metadata":{"id":"bZIUQbs8XLw5","executionInfo":{"status":"ok","timestamp":1677979583191,"user_tz":300,"elapsed":95,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracy_ = tf.keras.metrics.sparse_categorical_accuracy(real, pred)\n","    mask = tf.cast(mask, dtype=accuracy_.dtype)\n","    accuracy_ *= mask\n","    return tf.reduce_sum(accuracy_) / tf.reduce_sum(mask)\n"],"metadata":{"id":"bj_BbhMsXMXn","executionInfo":{"status":"ok","timestamp":1677979584482,"user_tz":300,"elapsed":116,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["input_dataset = tf.data.Dataset.from_tensor_slices(input_tensors)\n","output_dataset = tf.data.Dataset.from_tensor_slices(output_tensors)\n","\n","dataset= tf.data.Dataset.zip((input_dataset, output_dataset))\n","dataset = dataset.shuffle(buffer_size=500)"],"metadata":{"id":"Wsp37go3Y046","executionInfo":{"status":"ok","timestamp":1677979958589,"user_tz":300,"elapsed":94,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["validation_size = 0.2\n","num_elements = dataset.reduce(0, lambda x, _: x + 1).numpy() # count the number of elements in the dataset\n","num_validation = int(num_elements * validation_size)\n","\n","# creating training and validation dataset\n","train_dataset = dataset.skip(num_validation)\n","validation_dataset = dataset.take(num_validation)\n","\n","train_dataset = train_dataset.batch(batch_size)\n","validation_dataset = validation_dataset.batch(batch_size)"],"metadata":{"id":"WPUzM7PYZafC","executionInfo":{"status":"ok","timestamp":1677980622557,"user_tz":300,"elapsed":426,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","\n","transformer_model = Transformer(num_layers=6, embedding_dim=4, num_heads=4, fully_connected_dim=8,\n","                                input_vocab_size=ENCODER_VOCAB_SIZE, target_vocab_size=TARGET_VOCAB_SIZE,\n","                                max_pos_encoding_input=enc_seq_length,\n","                                max_pos_encoding_target=dec_seq_length)"],"metadata":{"id":"b5r74jJrbuMs","executionInfo":{"status":"ok","timestamp":1677980677493,"user_tz":300,"elapsed":351,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["\n","train_loss = Mean(name='train_loss')\n","train_accuracy = Mean(name='train_accuracy')\n","val_loss = Mean(name='val_loss')\n","# Create a checkpoint object and manager to manage multiple checkpoints\n","ckpt = train.Checkpoint(model=transformer_model, optimizer=optimizer)\n","ckpt_manager = train.CheckpointManager(ckpt, \"Weights\", max_to_keep=3)\n","\n","# Initialise dictionaries to store the training and validation losses\n","train_loss_dict = {}\n","val_loss_dict = {}\n","epochs = 50"],"metadata":{"id":"8w2dQ7sWd1D5","executionInfo":{"status":"ok","timestamp":1677982687121,"user_tz":300,"elapsed":89,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["@function\n","def train_step( encoder_input_data, decoder_target_data, decoder_output):\n","    with tf.GradientTape() as tape:\n","    # Run the forward pass of the model to generate a prediction\n","        prediction,_ = transformer_model(encoder_input, decoder_target_data, trainable=True)\n","        # Compute the training loss\n","        loss = loss_function(decoder_output, prediction)\n","        # Compute the training accuracy\n","        accuracy = compute_accuracy(decoder_output, prediction)\n","    # Retrieve gradients of the trainable variables with respect to the training loss\n","    gradients = tape.gradient(loss, transformer_model.trainable_weights)\n","    # Update the values of the trainable variables by gradient descent\n","    optimizer.apply_gradients(zip(gradients, transformer_model.trainable_weights))\n","    train_loss(loss)\n","    train_accuracy(accuracy)\n","    start_time = time()\n","    for epoch in range(epochs):\n","        train_loss.reset_states()\n","        train_accuracy.reset_states()\n","        val_loss.reset_states()\n","        # Iterate over the dataset batches\n","        for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n","            # Define the encoder and decoder inputs, and the decoder output\n","            encoder_input = train_batchX[:, 1:]\n","            decoder_input = train_batchY[:, :-1]\n","            decoder_output = train_batchY[:, 1:]\n","            train_step(encoder_input, decoder_input, decoder_output)\n","            # Print epoch number and loss value at the end of every epoch\n","            print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n","            + f\"Training Accuracy {train_accuracy.result():.4f}\")\n","            # Save a checkpoint after every five epochs\n","            if (epoch + 1) % 5 == 0:\n","                save_path = ckpt_manager.save()\n","                print(f\"Saved checkpoint at epoch {epoch+1}\")\n","          # Run a validation step after every epoch of training\n","        for val_batchX, val_batchY in validation_dataset:\n","          # Define the encoder and decoder inputs, and the decoder output\n","          encoder_input = val_batchX[:, 1:]\n","          decoder_input = val_batchY[:, :-1]\n","          decoder_output = val_batchY[:, 1:]\n","          # Generate a prediction\n","          prediction,_,_ = transformer_model(encoder_input, decoder_input, training=False)\n","          # Compute the validation loss\n","          loss = loss_function(decoder_output, prediction)\n","          val_loss(loss)\n","\n","        # Print epoch number and accuracy and loss values at the end of every epoch\n","        print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n","        + f\"Training Accuracy {train_accuracy.result():.4f}, \"\n","        + f\"Validation Loss {val_loss.result():.4f}\")\n","        # Save a checkpoint after every epoch\n","        if (epoch + 1) % 1 == 0:\n","          save_path = ckpt_manager.save()\n","          print(f\"Saved checkpoint at epoch {epoch+1}\")\n","          # Save the trained model weights\n","          transformer_model.save_weights(\"/Weights_\" + str(epoch + 1) + \".ckpt\")\n","          train_loss_dict[epoch] = train_loss.result()\n","          val_loss_dict[epoch] = val_loss.result()\n","    with open('./train_loss.pkl', 'wb') as file:\n","      dump(train_loss_dict, file)\n","    # Save the validation loss values\n","    with open('./val_loss.pkl', 'wb') as file:\n","      dump(val_loss_dict, file)\n","    print(\"Total time taken: %.2fs\" % (time() - start_time))"],"metadata":{"id":"m3z9Im81MDJA","executionInfo":{"status":"ok","timestamp":1677982665289,"user_tz":300,"elapsed":111,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["transformer_model.compile()\n","transformer_model.fit(train_dataset, validation_dataset)"],"metadata":{"id":"bsPdQ1SSj_ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, predicted_tf in enumerate(translation[:10]):\n","  print('-------------')\n","  hindi_pred = ' '.join([output_text_vectorizer.get_vocabulary()[np.argmax(pred)] for pred in predicted_tf.numpy()])\n","  print(f'English:{input_texts[i]}')\n","  print(f'Hindi:{target_texts[i]}')\n","  print(f'Predicted Hindi translation:{hindi_pred}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3CrosTsqPlI","executionInfo":{"status":"ok","timestamp":1677982693500,"user_tz":300,"elapsed":2260,"user":{"displayName":"Chaithra K.C","userId":"06590399226295405413"}},"outputId":"b35bd744-4392-4d03-e714-bfa592f99e4e"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------\n","English:<sos> wow! <eos>\n","Hindi:<sos> वाह! <eos>\n","Predicted Hindi translation:गर्मी खुली धूम्रपान डाँट खाती खोल खोल लगाऊँगा हूँ खाए तीस खाती खाती घर मीटिंग खाए लाती खाती खाती खाती मान धूम्रपान लाती तीस खाती खाती चल खुली इस खाती\n","-------------\n","English:<sos> duck! <eos>\n","Hindi:<sos> झुको! <eos>\n","Predicted Hindi translation:गर्मी जैसे फटाफट लाती खाती गर्मी खाती गर्मी बस्ता। वहां तीस खाती खाती वहां मीटिंग जीत खाती खाती खाती गर्मी अभिनेता खाए तीस खाती खाती खाती लगाऊँगा व्यवहार तीस खाती\n","-------------\n","English:<sos> duck! <eos>\n","Hindi:<sos> बतख़! <eos>\n","Predicted Hindi translation:खाती बस्ता। तीस लाती खाती खोल खोल लगाऊँगा मिल लाती खाती खाती खाती खाती हूँ तीस तीस खाती खाती खोल पढ़ूँ। डाँट तीस तीस खाती खाती लगाऊँगा बदतमीज़ इन्तज़ार खाती\n","-------------\n","English:<sos> help! <eos>\n","Hindi:<sos> बचाओ! <eos>\n","Predicted Hindi translation:मीटिंग मीटिंग धूम्रपान बदतमीज़ रात सदस्य घर अभिनेता डाँट खाए इन्तज़ार खाती गर्मी गर्मी मान खाए तीस खाती खाती खाती मान खुली इन्तज़ार खाती खाती खाती नाई जीत इन्तज़ार खाती\n","-------------\n","English:<sos> jump. <eos>\n","Hindi:<sos> उछलो. <eos>\n","Predicted Hindi translation:खाती हूँ धूम्रपान बदतमीज़ खोल खाती खाती लगाऊँगा फटाफट खाए इन्तज़ार तीस खाती घर हूँ जीत तीस खाती खाती खाती मीटिंग बदतमीज़ लाती खाती खाती खाती अभिनेता चल लाती खाती\n","-------------\n","English:<sos> jump. <eos>\n","Hindi:<sos> कूदो. <eos>\n","Predicted Hindi translation:खाती मिल चल इन्तज़ार खाती खोल खाती लगाऊँगा फटाफट कमाई खाती खाती खाती खोल खुली लाती खाती खाती खाती खाती पढ़ूँ। लाती तीस तीस खाती खोल सदस्य रात तीस खाती\n","-------------\n","English:<sos> jump. <eos>\n","Hindi:<sos> छलांग. <eos>\n","Predicted Hindi translation:खाती खाती खाती खाती खाती गर्मी गर्मी गर्मी फटाफट गए खाती खाती खाती गर्मी बस्ता। खाती खाती खाती खाती खाती मान तीस खाती खाती खाती गर्मी लगाऊँगा लाती खाती खाती\n","-------------\n","English:<sos> hello! <eos>\n","Hindi:<sos> नमस्ते। <eos>\n","Predicted Hindi translation:खाती खाती लाती तीस खाती खाती खाती नाई बदतमीज़ जीत तीस खाती खाती फैलाव धूम्रपान जीत तीस खाती खाती खोल खाती बदतमीज़ इन्तज़ार खाती खाती गर्मी पढ़ूँ। रात इन्तज़ार तीस\n","-------------\n","English:<sos> hello! <eos>\n","Hindi:<sos> नमस्कार। <eos>\n","Predicted Hindi translation:खाती मिल तीस तीस खाती खाती खाती गर्मी मीटिंग लाती खाती खाती खाती गर्मी मान तीस खाती खाती खाती खाती अभिनेता तीस तीस खाती खाती खाती चलाना तीस तीस खाती\n","-------------\n","English:<sos> cheers! <eos>\n","Hindi:<sos> वाह-वाह! <eos>\n","Predicted Hindi translation:गर्मी मीटिंग धूम्रपान लाती तीस खाती खोल मीटिंग तीस पट तीस खोल खोल खाती रोज़ जीत तीस खाती खाती खोल खोल व्यवहार तीस खाती खाती खाती फैलाव चल लाती खाती\n"]}]},{"cell_type":"markdown","source":["<a name='7'></a> \n","## 6 - References\n","\n","Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n","\n","\n","Francois Chollet. \"A ten-minute introduction to sequence-to-sequence learning in Keras\". Keras Blog, 14 September 2016, https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html.\n","\n","\n","\n","\n","Jalammar, J. (2018, August 24). The Illustrated Transformer. Retrieved from http://jalammar.github.io/illustrated-transformer/\n","\n","\n","\n"],"metadata":{"id":"HG_PEigXvqhN"}},{"cell_type":"markdown","source":["# Comparing with LSTM and RNN\n","\n","- The main idea behind Transformers is to replace recursive approach in RNNs by self-attnetion mechanism.\n","\n","- Self-attention mechanism allows the model to encode the input sequence by including the context by attending to different parts of the input sequence (via Query, Key, and Value tensors)\n","\n","- For example, if we want to translate the sentence \"I am a student\" to French, we would feed the input sequence one word at a time and update the hidden state at each time step. The final hidden state would be used to generate the output \"je suis étudiant.\"\n","\n","- In transformers, we would compute self-attention scores between all the words in the input sequence, and use these scores to compute a weighted sum of the input sequence to obtain a representation for each output element. This allows the model to consider all the words in the input sequence at once, and to capture long-range dependencies between them.\n","\n","\n","Advantages of Transformers:\n","- Parellel processing of the inputs; faster training\n","- Does not suffer from vanishing gradient or exploding gradient problem; makes it easy to train sequences"],"metadata":{"id":"QLsuXiKUxgfW"}}]}